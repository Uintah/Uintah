<?xml version="1.0" encoding="iso-8859-1" ?>

<!--% require("../../Utilities/Publish/srdoc.rb") %-->
<!--% doc = Doc.create(Doc::DocBook) %-->

<!--
   For more information, please see: http://software.sci.utah.edu

   The MIT License

   Copyright (c) 2004 Scientific Computing and Imaging Institute,
   University of Utah.

   License for the specific language governing rights and limitations under
   Permission is hereby granted, free of charge, to any person obtaining a
   copy of this software and associated documentation files (the "Software"),
   to deal in the Software without restriction, including without limitation
   the rights to use, copy, modify, merge, publish, distribute, sublicense,
   and/or sell copies of the Software, and to permit persons to whom the
   Software is furnished to do so, subject to the following conditions:

   The above copyright notice and this permission notice shall be included
   in all copies or substantial portions of the Software.

   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
   OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
   THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
   DEALINGS IN THE SOFTWARE.
-->


<!--% 
entities = {"sr"=>"SCIRun", "sr_bibtex_cit_url"=>"http://software.sci.utah.edu/scirun-biopse_citation.bib"}

doc.doIfHTML {
entities["sr_bibtex_cit"] = "javascript:function NewWindow(PageName,wide,tall,scroll){window.open(PageName,'','toolbar=0,location=0,directories=0,status=0,menubar=0,scrollbars=' + scroll + ',resizable=0,width=' + wide + ',height=' + tall + ',left=0,top=0');} NewWindow('&sr_bibtex_cit_url;', '600','400','1')"
entities["biopse_bibtex_cit"] = "javascript:function NewWindow(PageName,wide,tall,scroll){window.open(PageName,'','toolbar=0,location=0,directories=0,status=0,menubar=0,scrollbars=' + scroll + ',resizable=0,width=' + wide + ',height=' + tall + ',left=0,top=0');}NewWindow('&biopse_bibtex_cit_url;', '600','400','1')"
}

doc.insertDocType("article", entities) 
%-->

<article class="faq">
  <title>User FAQ (v. <!--%= doc.edition %-->)</title>

  <qandaset defaultlabel="itkosx103">

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>8/9/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  I get link errors when compiling ITK 2.0.1 OS X 10.3 to use with SCIRun.
	</para>
      </question>
      <answer>
	<para>
	  If building ITK 2.0.1 on OS X 10.3, users may see the following errors: <br/>
	  InsightToolkit-2.0.1/bin/libitkvnl.dylib...
	  ld: vnl_math.o illegal reference for -dynamic code (section   
	  difference reference from section (__TEXT,__eh_frame) relocation   
	  entry (0) to symbol: ___isnan defined in dylib: /usr/lib/libm.dylib)
	  /usr/bin/libtool: internal link edit command failed
	  make[9]: *** [/Users/blah/Code/C++/ITK/InsightToolkit-2.0.1/ 
	  bin/ libitkvnl.dylib] Error 1.
	</para>
	<para>
	  The solution is to either
	  (a) checkout a recent CVS copy of ITK (directions are on the ITK  
	  download page), or
	  (b) Edit the file:
	  [INSIGHT_ROOT]/Utilities/vxl/core/vnl/vnl_math.cxx
	  changing line 62 from:
	  # define isnan __isnan
	  to
	  # define isnan(x) __isnand((double)x)
	  
	  This simple fix will allow ITK 2.0.1 to compile on OS X.
	</para>
      </answer>
    </qandaentry>

<!-- ++++++++++++++++++++ -->

  <qandaset defaultlabel="number">

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>3/7/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  On a Linux system, what video cards and drivers work with
	  SCIRun's advanced volume rendering code?
	</para>
      </question>
      <answer>
	<para>
	  Nvidia FX class or better and ATI Radeon 9500 or better
	  video cards will work.
	</para>
	<para>
	  An accelerated graphics card driver must also be installed.
	  An accelerated graphics card driver offloads graphics
	  computations to the graphics card hardware and results in
	  faster graphics. Acclerated drivers enable scientific
	  imaging modalities, such as volume rendering, to happen on
	  the graphics card.
	</para>
	<para>
	  A <link linkend="card1">later</link> FAQ shows how to
	  determine the type of graphics card installed.
	</para>
	<para>
	  Accelerated Nvidia drivers can be <ulink
	  url="http://www.nvidia.com/content/drivers/drivers.asp">downloaded</ulink>
	  from Nvidia's web site.
	</para>
	<para>Accelerated ATI drivers can be
	  <ulink
	  url="https://support.ati.com/ics/support/KBAnswer.asp?questionID=3380">downloaded</ulink>
	  from ATI's web site.
	</para>
	<para>
	  Note that a non-accelerated driver is normally installed in
	  a typical Linux installation.  A <link
	  linkend="driver1">later</link> FAQ shows how to determine if
	  an accelerated driver is installed.
	</para>
      </answer>
    </qandaentry>

    <qandaentry id="card1">
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>3/7/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  On a Linux system, how do I determine the type of installed graphics card?
	</para>
      </question>
      <answer>
	<para>
	  There are two commands that can be used: lspci and glxinfo. The lspci command
	  can be used to identify the card even when no graphics drivers are installed,
	  but it may not report the exact model of the card correctly.
	</para>
	<para>
	  The glxinfo command will report the exact model of the card installed, but it
	  will only work if the graphics drivers are properly installed and Xwindows is
	  up and running. The glxinfo command 
	  must be run from an xterm that is being displayed on the machine's local monitor
	  (it cannot be run from a remote ssh login or from a text-only console).
	</para>
	<para>
	  Here is a sample run of lspci and glxinfo:
	  <literallayout class="monospaced">
$ /sbin/lspci | grep VGA
01:00.0 VGA compatible controller: nVidia Corporation NV35GL [GeForce FX 5900]  
$
$ cat /var/log/XFree86.0.log |grep -i GPU
(II) NVIDIA Unified Driver for all NVIDIA GPUs
(--) Chipset NVIDIA GPU found
(II) NVIDIA(0): NVIDIA GPU detected as: GeForce FX 5900 Ultra
(--) NVIDIA(0): Interlaced video modes are supported on this GPU
$
$ glxinfo |grep renderer
OpenGL renderer string: GeForce FX 5900 Ultra/AGP/SSE
	  </literallayout>
	  
	  If your card was detected when linux was loaded on the
	  machine, then a section in /etc/X11/XFree86Config, similar to the
	  following, will be found:
<literallayout class="monospaced">
Section "Device"
     Identifier  "Videocard0"
     Driver      "nvidia"
     VendorName  "Videocard vendor"
     BoardName   "NVIDIA GeForce 4 (generic)"
EndSection
</literallayout>
	</para>
	<para>
	  If there are multiple "Device" sections, look towards the end of the 
	  file for a section named "Screen" to determine which "Device" 
	  is currently being used.
	</para>
	<para>
	  Please note that there are multiple X11 implementations, and the exact
	  configuration file names, locations and formats may vary slightly depending
	  on your linux distribution. Please see
	  <ulink url="http://www.xfree86.org">XFree86</ulink> and 
	  <ulink url="http://www.x.org">X.org</ulink> for an up to date description
	  of the configuration files.
	</para>
      </answer>
    </qandaentry>

    <qandaentry id="driver1">
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>3/7/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  On a Linux system, how do I determine if an accelerated
	  graphics card driver is installed?
	</para>
      </question>
      <answer>
	<para>
	  Look in <filename>/etc/X11/XF86Config</filename> or
          <filename>/etc/X11/xorg.conf</filename> for a
	  section named <quote>Device</quote>. The accelerated ATI
	  driver is called <quote>fglrx</quote>:
	  <literallayout class="monospaced">
Section "Device"
    &#x22EE;
    Driver      "fglrx"  &lt;== accelerated
    Driver      "radeon" &lt;== non-accelerated
    Driver      "ati"    &lt;== non-accelerated
	  </literallayout>
	  The Accelerated Nvidia driver is called <quote>nvidia</quote>
	  <literallayout class="monospaced">
Section "Device"
    &#x22EE;
    Driver       "nvidia"  &lt;== accelerated
    Driver       "nv"      &lt;== non-accelerated
	  </literallayout>
	</para>
      </answer>
    </qandaentry>

    <qandaentry id="nvidiadriver1">
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>3/7/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  On a Linux system, how do I know which Nvidia driver is
	  currently installed?
	</para>
      </question>
      <answer>
	<para>
	  Look in /proc/driver/nvidia:
	  <literallayout class="monospaced">
$ cat /proc/driver/nvidia/version
NVRM version: NVIDIA Linux x86 NVIDIA Kernel Module  1.0-5336  Wed Jan 14 18:29:26 PST 2004
	  </literallayout>
Above, the driver version is 5336.
	</para>
	<para>
Or, run glxinfo
<literallayout class="monospaced">
$ glxinfo | grep version
server glx version string: 1.3
client glx version string: 1.3
OpenGL version string: 1.4.1 NVIDIA 53.36
glu version: 1.3
</literallayout>
	</para>
      </answer>
    </qandaentry>

    <qandaentry id="nvidiadriver2">
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>3/7/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  I installed the latest Nvidia drivers, why won't SCIRun
	  compile with advanced volume rendering?
	</para>
      </question>
      <answer>
	<para>
	  You need to install the Nvidia OpenGL header files (they
	  don't get installed by default). In order to install the
	  headers, you need to give the
	  <option>--opengl-headers</option> option to the install
	  script, for example:
	  <literallayout class="monospaced">
./NVIDIA-Linux-x86-1.0-6111-pkg1.run --opengl-headers
	  </literallayout>
	  For more help, type
	  <literallayout class="monospaced">
./NVIDIA-Linux-x86-1.0-6111-pkg1.run -A --help
	  </literallayout>
	  After installing the Nvidia OpenGL headers, you should find
	  files <filename>gl.h</filename>, <filename>glx.h</filename>,
	  <filename>glxtokens.h</filename> in directory
	  <filename>/usr/include/GL</filename>.  Also check to make
	  sure that those files are the Nvidia versions and not the
	  Mesa version (Mesa is a software-only OpenGL implementation
	  that comes packaged with linux):
	  <literallayout class="monospaced">
$ pwd
/usr/include/GL
$ head gl.h

* Mesa 3-D graphics library
	  </literallayout>
	  The above output indicates the presence of Mesa headers.
	  Install the Nvidia headers.
	  <literallayout class="monospaced">
$ head gl.h

** Copyright 1998-2002, NVIDIA Corporation.
	  </literallayout>
	  The above output shows that Nvidia headers are installed.
	</para>
      </answer>
    </qandaentry>

    <qandaentry id="nvidiadriver3">
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>3/7/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  How do I tell if the Nvidia headers are installed? How do I
	  know if the Nvidia headers match the version of Nvidia
	  driver I have installed?
	</para>
      </question>
      <answer>
	<para>
	  If in doubt, reinstall the driver. See the <link
	  linkend="nvidiadriver2">previous</link> FAQ entry.
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>02/22/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  What are the units associated with the bioelectric field
	  simulations in BioPSE?
	</para>
      </question>
      <answer>
	<para>
	  Here are the quantities that we're interested in and the
	  units that they carry:

	  <literallayout>
conductivity [sigma] = (amps / volts) / meter = siemens / meter
potentials [phi] = volts
electric field [E] = volts / meter
dipole source [p] = amps / meter
current density [J] = amps / (meter^2)
current source density [I] = amps / (meter^3)

Recall that:
  J = sigma x E
and
  E = - gradient (phi)
and
  I = divergence (J)
	  </literallayout>
	</para>
	<para>
	  As for the scales of those units (e.g. centi-, milli-, etc),
	  those are up to the user&#x2014;SCIRun doesn't actually keep
	  track of them all that well.  SCIRun assumes that the dipole
	  source are in amps / meter, that the conductivities are in
	  siemens / meter, that the model geometry is in meters, and
	  it therefore reports its results in volts.  If this isn't
	  true (e.g. you know that your model is in mm), then you have
	  to carry those scale factors through your computation
	  (e.g. know that the results you get out are in kv).
	</para>
	<para>
	  The rule of thumb is that if we
	  <emphasis>increase</emphasis> the conductivity, then we
	  <emphasis>decrease</emphasis> the potentials by that same
	  scale factor.  Conversely, if we
	  <emphasis>increase</emphasis> the dipole source moment, then
	  we <emphasis>increase</emphasis> the potentials by that same
	  scale factor.  i.e. V = I / sigma.  Similarly, if we hold
	  everything else constant, but <emphasis>increase</emphasis>
	  the size of the domain, then we
	  <emphasis>decrease</emphasis> the potentials by that same
	  scale factor.
	</para>
	<para>
	  For example, the SCIRun Utah Torso model has a length scale
	  of centimeters (10^(-2) meters) and conductivity units of
	  (siemens / meter), so our output potentials are in
	  hectovolts (10^2 volts).
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>01/27/05</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  I just upgraded SCIRun and I'm experiencing a number of bad
	  behaviors.  Any ideas?
	</para>
      </question>
      <answer>
	<para>
	  Try this: Rename <filename>~/.scirunrc</filename> to
	  <filename>~/.scirunrc.O</filename>.  Run SCIRun again.  If
	  the problems resolve then merge any changes you made in
	  <filename>~/.scirunrc.O</filename> into the new
	  <filename>~/.scirunrc</filename> created by SCIRun and
	  delete <filename>~/.scirunrc.O</filename>.
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>7/29/01</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  What systems are compatible with SCIRun?
	</para>
      </question>

      <answer>
	<para>
	  SCIRun should be compatible with any SGI machine running Irix 6.5. 
	</para>
	<para>
	  SCIRun has been tested on the following Linux distributions:
          (With past versions we have run on RedHat 7 and 8, and
           Mandrake 8.  While these systems should still work, all local
           systems have been updated to Redhat 9 and Mandrake 9.2.)
	  <itemizedlist>
	    <listitem>
	      <simpara>Mandrake 9.2</simpara>
	    </listitem>
	    <listitem>
	      <simpara>Redhat 9.0</simpara>
	    </listitem>
	  </itemizedlist> 
	</para>
	<para>
	  SCIRun has been tested on the following PC processor configurations:
	  <itemizedlist>
	    <listitem>
	      <simpara>Dual Intel Pentium II</simpara>
	    </listitem>
	    <listitem>
	      <simpara>Single Intel Pentium III</simpara>
	    </listitem>
	    <listitem>
	      <simpara>Dual Intel Pentium III</simpara>
	    </listitem>
	    <listitem>
	      <simpara>Single Intel Pentium 4</simpara>
	    </listitem>
	    <listitem>
	      <simpara>Single AMD Athlon</simpara>
	    </listitem>
	  </itemizedlist>
	</para>
	<para>
	  SCIRun has been tested with the following PC graphics cards:
	  <itemizedlist>
	    <listitem>
	      <simpara>NVIDIA GeForce &amp; FX</simpara>
	    </listitem>
            <listitem>
              <simpara>ATI Radeon 9700</simpara>
            </listitem>
	  </itemizedlist>
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>7/29/01</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  What is the relationship between SCIRun, BioPSE, and other SCI
	  Institute software?
	</para>
      </question>

      <answer>
	<para>
	  It is important to understand the hierarchy of computational problem solving
	  environments developed at the SCI Institute.  From a historical
	  perspective, SCIRun, which started development in 1992, was the
	  original implementation of the computational framework.  Since then,
	  SCIRun and its computational workbench infrastructure has been the
	  origin of many significant application-specific projects. Two major
	  examples are the DOE sponsored Uintah system and the NIH sponsored
	  BioPSE system (from the <ulink url="http://www.ncrr.nih.gov/">National
	  Center for Research Resources</ulink> (NCRR) Center at <ulink
	  url="http://www.sci.utah.edu/ncrr/">Utah</ulink>). The target
	  applications of the Uintah project are combustion, computational fluid
	  dynamics, and mechanical modeling implemented on large-scale,
	  distributed, shared memory architectures. The goal of the BioPSE
	  project is to create software for geometric modeling, simulation, and
	  visualization for solving bioelectric field problems.  A
	  secondary goal of the SCIRun system is to make source code for 
	  problem solving environments publicly available to the scientific
	  community.
	</para>
	<para>
	  To realize these significant projects, the SCIRun infrastructure
	  has required significant reorganization, extension, and
	  enhancement.  Even with these changes, SCIRun remains the
	  core infrastructure for problem solving environments, and the name
	  used to refer to the entire ensemble of software.  Thus, a user may
	  install and operate the core SCIRun software and also augment its
	  functionality with one or more of the <quote>packages</quote> such as
	  BioPSE.  SCI anticipates that the collection of packages will grow as
	  the advantages of the SCIRun infrastructure become available to
	  scientists and engineers of all disciplines.
	</para>
	
	<para>
	  <figure id="fig.biopsesr">
	    <title>BioPSE Software System</title>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="EAB-BioPSE.gif" format="GIF" width="645" align="center"/>
	      </imageobject>
	      <imageobject>
		<imagedata fileref="EAB-BioPSE.eps" format="EPS" align="center"/>
	      </imageobject>
	      <caption>
		<para>
		  This figure shows the relationship among SCIRun and its packages.
		  BioPSE consists of the basic SCIRun software together with the BioPSE
		  modules and support libraries.
		</para>
	      </caption>
	    </mediaobject>
	  </figure>
	</para>

	<para>
	  In addition to major projects that have both leveraged and
	  advanced SCIRun, there exist a number of smaller packages that 
	  extend SCIRun's utility. Examples include the Teem package for raster
	  data processing, the NetSolve package for linear algebra subroutines
	  (developed by researchers at the University of Tennessee and
	  Knoxville), and a communications interface recently introduced
	  to the Matlab program. SCI has developed various forms of software
	  wrappers or interfaces that allow SCIRun to leverage the strengths of
	third party tools, links referred to as "bridges."  </para>
	<para>
	  There are instances when a tighter level of integration than
	  a bridge between SCIRun and third-party software is necessary. One
	  example is the addition of mpeg support for capturing animations from
	  the SCIRun Viewer module. SCI uses the Berkeley and Alex
	  Knowles' mpeg encoding tools.  Another example is the set of image
	  generation and manipulation tools from Paul Haeberli called
	  libimage. To indicate if such tools are available, the
	  configure scripts for SCIRun contain optional control flags.
	</para>
	<para>
	  The combination of a robust infrastructure and
	  modular extensibility through packages and third-party libraries 
	  allows SCIRun to grow and adapt to changing needs and opportunities.
	</para>

      </answer>
    </qandaentry>

    <qandaentry>

      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>3/15/03</date>
	</revision>
      </revhistory>

      <question>
	<para>
	  What citation should be used when referring to SCIRun and BioPSE in a
	  paper or grant application?
	</para>
      </question>

      <answer>
	<para>
	  <quote>BibTeX</quote> and <quote>plain text</quote>
	  citations can be used.
<!--%=
doc.doIfHTML {<<HTML_TEXT
Here are SCIRun and BioPSE <ulink url="&sr_bibtex_cit;">citations</ulink>.
HTML_TEXT
}
%-->
<!--%=
doc.doIfPrint {<<PRINT_TEXT
For SCIRun and BioPSE citations go to: 
<literallayout class="monospaced">
&sr_bibtex_cit_url;
</literallayout>
PRINT_TEXT
}
%-->
	</para>
	<para>
	  Thank you for the citation(s).  Use of SCIRun/BioPSE citations 
	  in projects and publications helps ensure the continued
	  funding of the SCIRun/BioPSE project.  Please send
	  citations to macleod@sci.utah.edu.
	</para>
      </answer>

    </qandaentry>

    <qandaentry> 
      <question>
	<para> 
	  How do I get my data into SCIRun?
	</para> 
      </question> 

      <answer>
	<para>
	  A set of command line utilities, called
	  <firstterm>converters</firstterm>, convert <quote>foreign</quote> data
	  into SCIRun file objects (and vice-versa). See Section&#xA0;<ulink
	  url="(%= doc.treeUrl(%Q{doc/User/Guide/usersguide/srug7.html}) %)"><citetitle>Importing and
	  Exporting SCIRun Data</citetitle></ulink> in the SCIRun <citetitle>User's
	  Guide</citetitle> for information on the use of converter
	  programs.
	</para>
	<para>
	  See also <ulink url="(%=
	  doc.treeUrl(%Q{doc/User/Tutorials/BioFem/BioFem.html#appendix_a})
	  %)">Appendix A</ulink> of the BioFEM tutorial for
	  information about getting finite element models imported
	  into BioFEM.
	</para>
      </answer>

    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>7/29/01</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  By default,  what directions do the positive x,y,z axis point to in the
	  viewer module?
	</para>
      </question>
      <answer>
	<para>
	  By default, the negative-z axis points down, positive-x
	  points to right, and positive-y points up. If the
	  Axes is turned on, arrows point in +/-{x,y,z}. 
	  To remember which is which: (R,G,B) = (X,Y,Z).  Arrows
	  pointing in  positive directions are brighter than arrows 
	  pointing in  negative directions.  ???One last note, click
	  on the "Views" button at the botton-right of the ViewWindow and
	  snap to a "canonical" view (e.g. "Look down +Z
	  axis, Up vector +Y").???
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>7/29/01</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  Is there a way to scale up the size of the axes in the
	  viewer window to make the size of the axes comparable to
	  the size of other objects?
	</para>
      </question>
      <answer>
	<para>
	  Currently, everything must be scaled down to the size of unit-axes,
	  by using the Math::BuildTransform and Field::TransformField modules.
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>3/25/04</date>
	</revision>
      </revhistory>
      <question>
	<para>What 64bit hardware/OS is recommended? Will SCIRun compile
	on 64bit Opteron systems?</para>
      </question>
      <answer>
	<para>SCIRun does not yet (support in being investigated) run
	on Opteron.  SGI Irix is the only supported 64 bit
	platform. </para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/04</date>
	</revision>
      </revhistory>
      <question>
	<para>Does SCIRun handle anisotropy in FEM/FDM forward calculations?</para>
      </question>
      <answer>
	<para>SCIRun/BioPSE fully supports inhomogeneous,
	anisotropic conductivities for FEM and FDM simulations; an Nx6
	matrix can be built that provides a unique tensor for every
	element in the domain. See <ulink url="(%= doc.treeUrl(%Q{doc/User/Tutorials/BioFem/BioFem.html#appendix_a}) %)">Appendix A</ulink>
	of the BioFEM tutorial for more
	information about getting finite element models imported into
	BioPSE.
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/04</date>
	</revision>
      </revhistory>
      <question><para>Can SCIRun compile under Windows XP using a Cygwin
      Interface?</para>
      </question>
      <answer>
	<para>No. There has been discussion about creating a native
	Windows port of SCIRun, however, SCI has not committed to this
	project.</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/29</date>
	</revision>
      </revhistory>
      <question>
	<para>I built SCIRun under the latest release of Panther but the SCIRun
	executable doesn't do anything.  Why? </para>
      </question>
      <answer>
	<para>This problem appeared with the Mac OSX 10.3.2
	system update. To fix this problem, manually edit line 54 of file
	<filename>SCIRun/src/scripts/program.mk</filename>, inserting 
	"-bind_at_load" after "$(CXX)". Then
	remove the SCIRun executable and type gmake.</para>
	<para>This problem has been resolved in SCIRun release
	1.20.2.</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/04</date>
	</revision>
      </revhistory>
      <question>
	<para>The same descriptions are given for the
	ApplyFEMCurrentSource module and the ApplyFEMVoltageSource
	module. To which module does the description apply?</para>
      </question>
      <answer>
	<para>The difference between these modules is that the
	ApplyFEMCurrrentSource module is used to apply Neumann boundary
	conditions; known values related to the first spatial derivative
	of the potential, (e.g. flux, current density, dipoles). </para>
	<para>The ApplyFEMVoltageSource module is used to apply Dirichlet
	boundary conditions; known values for the potential at specific
	locations. In both cases, boundary conditions affect the right
	side of the linear system, which is why both modules take an
	optional RHS ColumnMatrix as input, and produce a modified RHS
	ColumnMatrix as output. In the case of a Dirichlet BC, the
	stiffness matrix must also be modified, which is why
	ApplyFEMVoltageSource requires the stiffness matrix as input, and
	produces a modified stiffness matrix as output.</para>
	<para>Typically, ApplyFEMCurrentSource takes two Fields as input:
	a TetVolField that specifies the geometry and conductivity
	information for the volume (e.g. brain-eg-mesh.tvt.fld), and a
	collection of dipole sources (PointCloudField&lt;Vector&gt;).</para>
	<para>In contrast, the ApplyFEMVoltageSource applies Dirichlet
	condition at location in the mesh where potentials are
	known. Unlike the ApplyFEMCurrentSource, the
	ApplyFEMVoltageSource requires a second upstream
	module. Specifically, an InsertVoltageSource module is
	required. The user passes the TetvolField of conductivity
	(e.g. brain-eg-mesh.tvt.fld) into the first Field input port
	of InsertVoltageSource, and a PointCloudField&lt;double>
	specifying the positions and voltages of the known potentials
	into the second Field input port. The module identifies which
	nodes of the mesh the Dirichlet conditions will be applied,
	and stores that information with the output TetvolField. The
	output TetVolField, which now has the Dirichlet BC information
	stored as a Property of the field, is passed into the first
	input port of ApplyFEMVoltageSource.
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/04</date>
	</revision>
      </revhistory>
      <question>
	<para>Following the SCIRun/BioPSE tutorial, I have set up
	several SCIRun networks. SCIRun, however, produces the
	following error messages. What do they mean?</para>
	<screen>
  DynamicLoader::compile_so() syscal error 256:
  command was 'cd/usr/local/SCIRun/linux/on-the-fly-libs;
  /usr/bin/gmake SFInterfaceMaker. TetVolFieldint.TetVolMeshCell.so>
  SFInterfaceMakerTetVolFieldint.TetVolMeshCell.log 2>&amp;1'
  DynamicLoader::create_cc(empty=true ) ' Could not create file
  /usr/local/SCIRun/linux/on-the-fly-libs/SFInterfaceMaker.TetVolFieldint.TetVolMeshCell.cc
	</screen>
      </question>
      <answer>
	<para>One feature of SCIRun/BioPSE is that it dynamically
	compiles some parts of the code at run-time based on the data
	passed through the network.  If installed from an RPM (and
	unless instructed otherwise) SCIRun will try to generate code
	in
	<filename>/usr/local/SCIRun/linux/on-the-fly-libs/</filename>.
	SCIRun won't (usually) have privilages to write to this
	directory.  The issue is fixed by creating a directory in your
	home directory, and setting the
	<envar>SCIRUN_ON_THE_FLY_LIBS</envar> environment variable to
	point to the new directory.  For example:
	<literallayout class="monospaced">
  mkdir /home/dmw/on-the-fly-libs/
  setenv SCIRUN_ON_THE_FLY_LIBS /home/dmw/on-the-fly-libs/
	</literallayout>
	SCIRun must be restarted to effect these changes. 
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/04</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  When I remotely display SCIRun, it sometimes hangs and sometimes
	  crashes the X server, depending on if it is running on sgi/linux, and
	  displayed on sgi/linux. What causes this?
	</para>
      </question>
      <answer>
	<para>Problems with the 4191 Nvidia drivers cause GL programs to
	misbehave. There has been success using the 3xxx Drivers (except 3D
	texturing). Determine what drivers are being used on the linux box
	by typing "rpm -qa grep NVIDIA".</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/04</date>
	</revision>
      </revhistory>
      <question>
	<para>In the GenStandardColorMaps module, I can not change the
	alpha value of the applied color map. Documentation claims the
	default alpha value for every map is 50%, but I see no
	transparency with the default setting. Changing settings has
	no visible effect. Should I enable another setting for the
	alpha value in the color map?
	</para>
      </question>
      <answer>
	<para>In order for alpha to be applied, the downstream module
	needs to have the "Enable Transparency" box selected under the
	"Faces" tab.
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/04</date>
	</revision>
      </revhistory>
      <question>
	<para>Is there an easy way to find the second spatial derivative
	in a volume?</para>
	
      </question>
      <answer>
	<para>No.</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>001</revnumber>
	  <date>03/29/04</date>
	</revision>
      </revhistory>
      <question>
	<para>Does SCIRun support 2D visualization?</para>
      </question>
      <answer>
	<para>Yes. The ShowSlices module will display orthogonal slices of
	      a 3D nrrd in a 2D OpenGL window.</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>8/27/03</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  I've installed SCIRun from its RPM.  I'm running the networks in the
	  tutorial and I'm getting these error messages and others like them:
	  <literallayout class="monospaced">
	    <![CDATA[
  REMARK: Maybe dynamically compiling some code, ignore failure here.
  DynamicLoader::create_cc(empty = false) - Could not create file
  /usr/local/SCIRun/linux/on-the-fly-libs/VFInterfaceMaker.TetVolFieldint.Te
  tVolMeshCell.cc
  DynamicLoader - Executing: cd /usr/local/SCIRun/linux/on-the-fly-libs;
  /usr/bin/gmake VFInterfaceMaker.TetVolFieldint.TetVolMeshCell.so
  DynamicLoader::compile_so() syscal error 256: command was 'cd
  /usr/local/SCIRun/linux/on-the-fly-libs; /usr/bin/gmake
  VFInterfaceMaker.TetVolFieldint.TetVolMeshCell.so >
  VFInterfaceMaker.TetVolFieldint.TetVolMeshCell.log 2>&1'
  DynamicLoader::create_cc(empty = true) - Could not create file
  /usr/local/SCIRun/linux/on-the-fly-libs/VFInterfaceMaker.TetVolFieldint.Te
  tVolMeshCell.cc
  DynamicLoader - Executing: cd /usr/local/SCIRun/linux/on-the-fly-libs;
  /usr/bin/gmake VFInterfaceMaker.TetVolFieldint.TetVolMeshCell.so
  DynamicLoader::compile_so() syscal error 256: command was 'cd
  /usr/local/SCIRun/linux/on-the-fly-libs; /usr/bin/gmake
  VFInterfaceMaker.TetVolFieldint.TetVolMeshCell.so >
  VFInterfaceMaker.TetVolFieldint.TetVolMeshCell.log 2>&1'
  DYNAMIC COMPILATION ERROR:
  /usr/local/SCIRun/linux/on-the-fly-libs/VFInterfaceMaker.TetVolFieldint.Te
  tVolMeshCell.so does not compile!!
  /usr/local/SCIRun/linux/on-the-fly-libs/VFInterfaceMaker.TetVolFieldint.Te
  tVolMeshCell.so: cannot open shared object file: No such file or directory
  REMARK: Dynamic compilation completed.
	    ]]>
	  </literallayout>
	</para>
      </question>
      <answer>
	<para>
	  One of the features of SCIRun/BioPSE is that it dynamically compiles
	  some parts of the code at run-time based on the data that you're
	  passing through your network.  In order for this to work, the software
	  needs to be able to dynamically create files.  However, because you
	  have installed the software from RPM, the directory that the dynamic
	  files are wrtten to by default, <filename>/usr/local/SCIRun/linux/on-the-fly-libs/</filename>, is owned by root&#x2014;which
	  is why the program is having problems.  You can  fix this by
	  creating a different directory that you own, and setting your
	  <envar>SCIRUN_ON_THE_FLY_LIBS</envar> environment variable to point at that new
	  directory.  This environment variable overrides the default location,
	  so that should fix the problem you're having.  For example:
	  <literallayout class="monospaced">
	    mkdir /home/dmw/on-the-fly-libs/
	    setenv SCIRUN_ON_THE_FLY_LIBS /home/dmw/on-the-fly-libs/
	</literallayout></para>
      </answer>
    </qandaentry>

    <qandaentry id="auto_exec_qa">
      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>3/18/03</date>
	</revision>
      </revhistory>
      <question>
	<para>Can a scirun net automatically
	execute as soon as it loads?</para>
      </question>
      <answer>
	<para>Add the following line to the end of the net file:
	<programlisting>
	  $m1-c needexecute 
	</programlisting>
	Adding the above line causes the execution of module
	<varname>$m1</varname>, plus the execution of all of modules that <varname>$m1</varname> depends on, and the execution of all modules that depend on <varname>$m1</varname>.
	</para>
      </answer>
    </qandaentry>
    <qandaentry>

      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>2/20/03</date>
	</revision>
      </revhistory>

      <question>
	<para>
	  When SCIRun is remotely displayed, it sometimes hangs and sometimes
	  crashes the X server (depending on whether it is running on sgi/linux
	  and being displayed on sgi/linux.)  What causes this?
	</para>
      </question>

      <answer>
	<para>There are problems with the 4191 Nvidia drivers, causing 
	GL programs to behave badly.  There has been success using the 3123
	drivers (except for 3D texturing).  Determine what drivers 
	are being used on the linux box by typing "rpm -qa | grep NVIDIA".
	</para>
	<para>
	  Since 4191 is the most recent driver on nvidia's
	  <ulink
	   url="http://www.nvidia.com/view.asp?IO=linux_display_archive">web
	   site</ulink>, go back and use the 3123 drivers.  NVIDIA does not have prebuilt
	   rpms for RedHat 8.0, but it is easy to build from the source rpms:
	   <literallayout class="monospaced">
	     # Removes the old installation.  The actual number may need to be changed
	     rpm -e NVIDIA_GLX-1.0-4191 NVIDIA_kernel-1.0-4191

	     # Build the rpm from the source
	     rpmbuild --rebuild NVIDIA_kernel-1.0-3123.src.rpm

	     # Install the rpm.  Note: the actual path may differ from system to system.
	     rpm -ivh /usr/src/redhat/RPMS/i386/NVIDIA_kernel-1.0-3123.i386.rpm

	     # The GLX rpm does not have to build from source (not kernel dependant),
	     # but I did anyway.
	     rpmbuild --rebuild NVIDIA_GLX-1.0-3123.src.rpm
	     rpm -ivh /usr/src/redhat/RPMS/i386/NVIDIA_GLX-1.0-3123.i386.rpm
	   </literallayout>
	</para>

      </answer>
    </qandaentry>

    <qandaentry>

      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>1/31/03</date>
	</revision>
      </revhistory>

      <question>
	<para>SCIRun dies with a memory allocation error.  Specifically:
	<literallayout class="monospaced">
	  Error allocating memory (32833536 bytes requested) mmap: errno=12 Thread
	</literallayout>
	</para>
      </question>

      <answer>
	<para>If SCIRun is not configured with
	<option>--enable-64bit</option>, the program will not use more
	than approximately 2G of memory.</para>
      </answer>

    </qandaentry>

    <qandaentry>

      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>9/15/02</date>
	</revision>
      </revhistory>

      <question>
	<para>
	  How are inverse problems in electrocardiography solved in
	  SCIRUN/BIOPSE. The so-called transfer matrix T is built and
	  then a least-squares problem is solved. The transfer matrix
	  is expressed using inverses of submatrices. Is this matrix T
	  effectively computed? Are inverses really computed ?  What
	  is the method implemented? In which modules can the source
	  code be seen?
	</para>
      </question>

      <answer>
	<para>
	  There are two general solutions to the bioelectric inverse
	  problem.  One formulation tries to recover equivalent dipole
	  sources. The other tries to recover the voltages on an
	  interior surface that is assumed to encompass any source
	  (i.e. there are no sources located between the measurement
	  surface and the interior surface).
	</para>
	<para>
	  In the first formulation (surface-to-source inversion), the
	  inverse problem can be over-constrained (searching for a
	  single dipole that accounts for the outer surface
	  measurements), or under-constrained (searching for a large
	  number of dipoles that are distributed through the domain).
	  The over-constrained case is called parameterized inversion,
	  The under-constrained case is called non-parameterized.  In
	  the parameterized case, a search algorithm locates the
	  dipole that best reproduces the measured data. In the
	  non-parameterized case, find the minimum-norm solution that
	  best fits some a-priori information about the data (e.g. the
	  solution should be spatially focused).
	</para>
	<para>
	  The inverse problem is ill-conditioned because of the
	  poorly-specified boundary conditions on the inner surface.
	  The formulation uses the transfer matrix built from a
	  combination of sub-matrices from the stiffness matrix.
	  SCIRun solves the resulting linear systems, but not by
	  computing an explicit inverse. Rather, an iterative
	  (conjugate-gradient) algorithm is used. There is a
	  preliminary version of this code, unfortunately, it has not
	  yet been released in BioPSE.  For the moment, see <ulink
	  url="http://www.sci.utah.edu/publications/spie95.ps.Z">this
	  SPIE `95 paper</ulink>, which describes this technique in
	  more detail.  This algorithm will be part of BioPSE in early
	  Spring.
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>1.0</revnumber>
	  <date>7/29/01</date>
	</revision>
      </revhistory>
      <question>
	<para>
	  When running SCIRun, one or more messages appear in the
	  message window indicating a problem with a package and/or
	  module.  The message(s) is/are similar to the following:
	  <literallayout class="monospaced">
	    Loading package 'SCIRun' Unable to load module 'CastField' :
	    - can't find symbol 'make_CastField'
	  </literallayout>
	  or
	  <literallayout class="monospaced">
	    Unable to load all of package 'Teem' (category 'DataIO'
	    failed) : - libPackages_Teem_Dataflow.so: cannot open
	    shared object file: No such file or directory -
	    libPackages_Teem_Dataflow_Modules_DataIO.so: cannot open
	    shared object file: No such file or directory
	  </literallayout>
	</para>
      </question>

      <answer>
	<para>
	  Each module for a given package has its own .xml file that
	  describes it. When SCIRun starts, it parses all .xml files in
	  the packages (under Dataflow/XML) and tries to find the
	  matching code within the related .so files. If the .so files
	  cannot be found, the message <quote>No such file or
	  directory</quote>" is given. If the .so can be found, but
	  the code for a particular module does not exist within the
	  library, the message <quote>can't find symbol</quote> is
	  given.
	</para>
	<para>
	  The message(s) may or may not indicate a problem with SCIRun. For some
	  modules, the .xml file may be listed, but the code has not been
	  completed (this may be common for module developers). For other modules,
	  the SCIRun installation is some how corrupt (.so files
	  have been deleted, moved, etc.)
	</para> 
	<para>
	  The solution is to build  libraries 
	  the .xml files need or remove the offending .xml files.
	</para>
      </answer>
    </qandaentry>

    <qandaentry>
      <revhistory>
	<revision>
	  <revnumber>1.0.1</revnumber>
	  <date>4/30/04</date>
	</revision>
      </revhistory>
      <question>
	<para>I am seeing the error <quote>Xlib: sequence
	lost&#x2026;</quote>when running SCIRun</para>
	<para>What does it mean and how do I fix it?</para>
      </question>
      <answer>
	<para>
	  The most likely reason for seeing this is that you have
	  found a bug in SCIRun with the locking of the X display.
	  You should report this problem to
	  scirun-develop@sci.utah.edu.  The last time this error
	  occurred it was in
	  <filename>Dataflow/Modules/Render/OpenGL.cc</filename> and
	  required that a glGetIntegerv() call be placed within a
	  gui->lock()/unlock() section.
	</para>
      </answer>
    </qandaentry>

  </qandaset>

</article>
