
_______________________________________________________________________________
AnalyticNonbonded.cc 07/09/13
  Setup method when neighbor lists were used
-------------------------------------------------------------------------------
void AnalyticNonbonded::setup(const ProcessorGroup* pg,
                              const PatchSubset* patches,
                              const MaterialSubset* materials,
                              DataWarehouse* old_dw,
                              DataWarehouse* new_dw)
{
  // create neighbor list for each atom in the system
  size_t numPatches = patches->size();
  size_t numMatls = materials->size();
  for (size_t p = 0; p < numPatches; ++p) {
    const Patch* patch = patches->get(p);
    for (size_t m = 0; m < numMatls; ++m) {
      int matl = materials->get(m);

      // get particles within bounds of current patch (interior, no ghost cells)
      ParticleSubset* local_pset = old_dw->getParticleSubset(matl, patch);

      // get particles within bounds of cutoff radius
      ParticleSubset* neighbor_pset = old_dw->getParticleSubset(matl, patch, Ghost::AroundNodes, CUTOFF_RADIUS, d_lb->pXLabel);

      constParticleVariable<Point> px_local;
      constParticleVariable<Point> px_neighbors;
      old_dw->get(px_local, d_lb->pXLabel, local_pset);
      old_dw->get(px_neighbors, d_lb->pXLabel, neighbor_pset);

      int patchIdx = patch->getID();
      generateNeighborList(local_pset, neighbor_pset, px_local, px_neighbors, d_neighborList[patchIdx]);

    }  // end material loop
  }  // end patch loop
}
_______________________________________________________________________________



_______________________________________________________________________________
Misc thread barrier code construct for use in Uintah 07/09/13
Shouldn't need this though.
-------------------------------------------------------------------------------
pthread_barrier_t barrier;
pthread_barrier_init(&barrier, NULL, Parallel::getNumThreads());
pthread_barrier_wait(&barrier);
pthread_barrier_destroy(&barrier);
_______________________________________________________________________________



_______________________________________________________________________________
SPME.cc 06/27/13
-------------------------------------------------------------------------------

void SPME::setup_old(const ProcessorGroup* pg,
                 const PatchSubset* patches,
                 const MaterialSubset* materials,
                 DataWarehouse* old_dw,
                 DataWarehouse* new_dw)
{
  size_t numPatches = patches->size();
  size_t numMatls = materials->size();
  for (size_t p = 0; p < numPatches; ++p) {
    const Patch* patch = patches->get(p);
    for (size_t m = 0; m < numMatls; ++m) {
      int matl = materials->get(m);

      Vector totalCellExtent = (d_system->getCellExtent()).asVector();
      Vector patchLowIndex = (patch->getCellLowIndex()).asVector();
      Vector patchHighIndex = (patch->getCellHighIndex()).asVector();

      Vector kReal = d_kLimits.asVector();

      SCIRun::IntVector patchKLow, patchKHigh;
      for (size_t idx = 0; idx < 3; ++idx) {
        patchKLow[idx]  =  ceil(kReal[idx] * (patchLowIndex[idx]  / totalCellExtent[idx]));
        patchKHigh[idx] = floor(kReal[idx] * (patchHighIndex[idx] / totalCellExtent[idx]));
      }

      IntVector patchKGridExtents = (patchKHigh - patchKLow);  // Number of K Grid points for this local patch
      IntVector patchKGridOffset = patchKLow;
      // Lowest K Grid point vector
      int splineSupport = d_interpolatingSpline.getSupport();
      IntVector  plusGhostExtents = IntVector(splineSupport, splineSupport, splineSupport);
      IntVector minusGhostExtents = IntVector(0, 0, 0);  // All ghosts are in the positive direction for shifted splines


      SPMEPatch* spmePatch = new SPMEPatch(patchKGridExtents, patchKGridOffset, plusGhostExtents, minusGhostExtents, patch);

      // Check to make sure plusGhostExtents+minusGhostExtents is right way to enter number of ghost cells (i.e. total, not per offset)
      SimpleGrid<dblcomplex>* q_patchLocal = scinew SimpleGrid<dblcomplex>(patchKGridExtents, patchKGridOffset, splineSupport);
      q_patchLocal->initialize(std::complex<double>(0.0, 0.0));

      // No ghost cells; internal only
      SimpleGrid<Matrix3>* stressPrefactor = scinew SimpleGrid<Matrix3>(patchKGridExtents, patchKGridOffset, 0);
      calculateStressPrefactor(stressPrefactor, patchKGridExtents, patchKGridOffset);

      // No ghost cells; internal only
      SimpleGrid<double>* fTheta = scinew SimpleGrid<double>(patchKGridExtents, patchKGridOffset, 0);
      fTheta->initialize(0.0);

      // Calculate B and C - we should only have to do this if KLimits or the inverse cell changes
      SimpleGrid<double> fBGrid = calculateBGrid(patchKGridExtents, patchKGridOffset);
      SimpleGrid<double> fCGrid = calculateCGrid(patchKGridExtents, patchKGridOffset);

      // Composite B and C into Theta
      size_t xExtent = patchKGridExtents.x();
      size_t yExtent = patchKGridExtents.y();
      size_t zExtent = patchKGridExtents.z();
      for (size_t xidx = 0; xidx < xExtent; ++xidx) {
        for (size_t yidx = 0; yidx < yExtent; ++yidx) {
          for (size_t zidx = 0; zidx < zExtent; ++zidx) {

            // Composite B and C into Theta
            (*fTheta)(xidx, yidx, zidx) = fBGrid(xidx, yidx, zidx) * fCGrid(xidx, yidx, zidx);

//            if (spme_dbg.active()) {
//              cerrLock.lock();
//              // looking for "nan" values interspersed in B and C
//              if (std::isnan(fBGrid(xidx, yidx, zidx))) {
//                std::cout << "B: " << xidx << " " << yidx << " " << zidx << std::endl;
//                std::cin.get();
//              }
//              if (std::isnan(fCGrid(xidx, yidx, zidx))) {
//                std::cout << "C: " << xidx << " " << yidx << " " << zidx << std::endl;
//                std::cin.get();
//              }
//              cerrLock.unlock();
            }
          }
        }
      }
      spmePatch->setTheta(fTheta);
      spmePatch->setStressPrefactor(stressPrefactor);
      spmePatch->setQ(q_patchLocal);

      d_spmePatchLock.writeLock();
      d_spmePatches.push_back(spmePatch);
      d_spmePatchMap.insert(pair<PatchMaterialKey, int>(PatchMaterialKey(patch, matl), p));
      d_spmePatchLock.writeUnlock();

    }  // end material loop
  }  // end patch loop
}

std::vector<Point> SPME::calcReducedCoords(const std::vector<Point>& localRealCoordinates,
                                           const MDSystem& system)
{
  std::vector<Point> localReducedCoords;
  size_t idx;
  Point coord;  // Fractional coordinates; 3 - vector

  // bool Orthorhombic; true if simulation cell is orthorhombic, false if it's generic
  size_t numParticles = localRealCoordinates.size();
  Matrix3 inverseBox = system.getCellInverse();        // For generic coordinate systems
  if (!system.isOrthorhombic()) {
    for (idx = 0; idx < numParticles; ++idx) {
      coord = localRealCoordinates[idx];                   // Get non-ghost particle coordinates for this cell
      coord = (inverseBox * coord.asVector()).asPoint();   // InverseBox is a 3x3 matrix so this is a matrix multiplication = slow
      localReducedCoords.push_back(coord);                 // Reduced non-ghost particle coordinates for this cell
    }
  } else {
    for (idx = 0; idx < numParticles; ++idx) {
      coord = localRealCoordinates[idx];        // Get non-ghost particle coordinates for this cell
      coord(0) *= inverseBox(0, 0);
      coord(1) *= inverseBox(1, 1);
      coord(2) *= inverseBox(2, 2);               // 6 Less multiplications and additions than generic above
      localReducedCoords.push_back(coord);      // Reduced, non-ghost particle coordinates for this cell
    }
  }
  return localReducedCoords;
}

_______________________________________________________________________________

    //    friend std::ostream& operator<<(std::ostream& out,
    //                                    const Uintah::SimpleGrid<T>& sg);
    
---------------------------------------------------------------------------------------------------

// Old code from monolithic SPME::Calculate()
bool converged = false;
int numIterations = 0;
int maxIterations = d_system->getMaxIterations();
while (!converged && (numIterations < maxIterations)) {

  std::vector<SPMEPatch*>::iterator iter;
  for (iter = d_spmePatches.begin(); iter != d_spmePatches.end(); iter++) {
    SPMEPatch* spmePatch = *iter;

    const Patch* patch = spmePatch->getPatch();
    ParticleSubset* pset = old_dw->getParticleSubset(materials->get(0), patch);

    constParticleVariable<Point> px;
    constParticleVariable<long64> pids;
    constParticleVariable<double> pcharge;
    old_dw->get(px, d_lb->pXLabel, pset);
    old_dw->get(pids, d_lb->pParticleIDLabel, pset);
    old_dw->get(pcharge, d_lb->pChargeLabel, pset);

    std::vector<SPMEMapPoint> gridMap = generateChargeMap(pset, px, pids, d_interpolatingSpline);
    mapChargeToGrid(spmePatch, gridMap, pset, pcharge, d_interpolatingSpline.getHalfMaxSupport());  // Calculate Q(r)

    SimpleGrid<complex<double> > Q = spmePatch->getQ();
    SimpleGrid<double> fTheta = spmePatch->getTheta();
    Q.initialize(complex<double>(0.0, 0.0));
    SimpleGrid<Matrix3> stressPrefactor = spmePatch->getStressPrefactor();

    // Map the local patch's charge grid into the global grid and transform
    //      SPME::GlobalMPIReduceChargeGrid(Ghost::AroundNodes);  //Ghost points should get transferred here
    //      SPME::ForwardTransformGlobalChargeGrid();  // Q(r) -> Q*(k)

    // Once reduced and transformed, we need the local grid re-populated with Q*(k)
    //      SPME::MPIDistributeLocalChargeGrid(Ghost::None);

    // Multiply the transformed Q out
    IntVector localExtents = spmePatch->getLocalExtents();
    size_t xExtent = localExtents.x();
    size_t yExtent = localExtents.y();
    size_t zExtent = localExtents.z();
    double localEnergy = 0.0;//Maybe should be global?
    Matrix3 localStress(0.0);//Maybe should be global?
    for (size_t kX = 0; kX < xExtent; ++kX) {
      for (size_t kY = 0; kY < yExtent; ++kY) {
        for (size_t kZ = 0; kZ < zExtent; ++kZ) {
          SimpleGrid<double> fTheta = spmePatch->getTheta();
          complex<double> gridValue = Q(kX, kY, kZ);

          // Calculate (Q*Q^)*(B*C)
          Q(kX, kY, kZ) = gridValue * conj(gridValue) * fTheta(kX, kY, kZ);
          localEnergy += std::abs(Q(kX, kY, kZ));
          localStress += std::abs(Q(kX, kY, kZ)) * stressPrefactor(kX, kY, kZ);
        }
      }
    }

    // Transform back to real space
    //      SPME::GlobalMPIReduceChargeGrid(Ghost::None);  //Ghost points should NOT get transferred here
    //      SPME::ReverseTransformGlobalChargeGrid();
    //      SPME::MPIDistributeLocalChargeGrid(Ghost::AroundNodes);

    //  This may need to be before we transform the charge grid back to real space if we can calculate
    //    polarizability from the fourier space component
    converged = true;
    if (d_polarizable) {
      // calculate polarization here
      // if (RMSPolarizationDifference > PolarizationTolerance) { ElectrostaticsConverged = false; }
      std::cerr << "Error:  Polarization not currently implemented!";
    }
    // Sanity check - Limit maximum number of polarization iterations we try
    ++numIterations;
  }
  //    SPME::GlobalReduceEnergy();
  //    SPME::GlobalReduceStress();  //Uintah framework?
}




#if !defined(__digital__) || defined(__GNUC__)
  template<>
#endif
  void ReductionVariable<LinearArray3<dblcomplex>, Reductions::Sum<LinearArray3<dblcomplex> > >::getMPIData(vector<char>& data,
                                                                                                            int& index)
  {
    int count = value.dim1() * value.dim2() * value.dim3();
    ASSERTRANGE(index, 0, static_cast<int>( (data.size() + 1) - (count * sizeof(std::complex<double>)) ));

    std::complex<double>* ptr = reinterpret_cast<dblcomplex*>(&data[index]);
    long int size = value.getSize();
    for (long idx = 0; idx < size; ++idx) {
      *ptr++ = value.get_dataptr()[idx];
    }
  }

#if !defined(__digital__) || defined(__GNUC__)
  template<>
#endif
  void ReductionVariable<LinearArray3<dblcomplex>, Reductions::Sum<LinearArray3<dblcomplex> > >::putMPIData(vector<char>& data,
                                                                                                            int& index)
  {
    int count = value.dim1() * value.dim2() * value.dim3();
    ASSERTRANGE(index, 0, static_cast<int>( (data.size() + 1) - (count * sizeof(std::complex<double>)) ));

    std::complex<double>* ptr = reinterpret_cast<dblcomplex*>(&data[index]);
    long size = value.getSize();
    for (long idx = 0; idx < size; ++idx) {
      value.get_dataptr()[idx] = *ptr++;
    }
  }

#if !defined(__digital__) || defined(__GNUC__)
  template<>
#endif
  void ReductionVariable<LinearArray3<dblcomplex>, Reductions::Sum<LinearArray3<dblcomplex> > >::getMPIInfo(int& count,
                                                                                                            MPI_Datatype& datatype,
                                                                                                            MPI_Op& op)
  {
    datatype = MPI_C_DOUBLE_COMPLEX;
    count = value.getSize();
    op = MPI_SUM;
  }
  
---------------------------------------------------------------------------------------------------  
  
task->setType(Task::OncePerProc);
LoadBalancer* loadBal = sched->getLoadBalancer();
GridP grid = level->getGrid();
const PatchSet* perprocPatches = loadBal->getPerProcessorPatchSet(grid);
sched->addTask(task, perprocPatches, d_sharedState->allMaterials());

---------------------------------------------------------------------------------------------------


