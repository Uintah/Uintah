#!/bin/bash
######## MOAB TEMPLATE SCRIPT #########
# Steve Brown, updated September 2008
#
# To make this work,
# 1) Specify filesystem lscratch{a,b,c} etc. 
# 2) Change project name in two places (Moab and Bash parts)
# 3) Look at handy-dandy reference tables and fill in values for
#    Job time, number of nodes, and processors per node 
# 4) Understand what's happening so you can fix it when it breaks.
# 5) Submit the job several times.  Daisy-chain them with dependencies
#     so no more than one job is running at a time.  The syntax is:
#	msub myCoolJob.moab -l depend=afterok:<previous-job's-ID>

####### JOB LIMITS ################### (12h493n means 493 nodes for 12 hours)
# QUEUE  WHEN	 ALC	 ATLAS	THUNDER	  UP	YANA	 ZEUS
# pbatch Wkdy	8h 296n	16h268n	12h493n	12h 32n	200h	12h 64n
# pbatch Wkend	24h296n	24h268n	24h493n	12h 32n	200h	24h 64n
# pdebug	30m 8n	30m 16n	30m 16n	30m 2n	30m	30m 12n
# Procs/Node	2	8	4	8	8	8
# RAM / Node	4 GiB	16 GiB	8 GiB	32 GiB	16/32G	16 GiB
# Filesystems	Lustre	Lustre	Lustre	/p/gup1	Lustre	Lustre

####### USER CONFIGURATION ###########
#MSUB -l partition=zeus         # Machine
#MSUB -l nodes=36 		# Nodes
#MSUB -q pbatch			# Specify pbatch, pdebug, or utahdat
#MSUB -l walltime=12:00:00     	# dd:hh:mm:ss	
#MSUB -m be			# Mail at beginning & end
#MSUB -v			# Export variables to environment
#MSUB -N jp8fire_zeus_003      	# This single Moab Job Name
PROJNAME=jp8fire		# Master project name
FILESYSTEM=lscratcha		# Specify lscratcha, lscratchb, or lscratchc
#####################################

## XXX Need more intelligent logging here (keep with output instead of spatter)
##MSUB -d /p/lscratcha/${USER}/${JOBNAME} # Job Execution dir
#MUSB -j			# Join Moab's stdout and stderr to the same file
##MSUB -o /p/lscratcha/${USER}/${JOBNAME}/logs/${JOBNAME}.o${JOBID}	# Moab's output file


#usually, SLURM_TASKS_PER_NODE is something intelligible like "8".
#On some hosts (ALC) it is set to "2(x16)" which needs to be fixed.
SLURM_CPN=`echo ${SLURM_TASKS_PER_NODE} | cut -f 1 -d '('` 
SLURM_NTASKS=`expr ${SLURM_NNODES} "*" ${SLURM_CPN}`
MACHINE=`echo ${SLURM_NODELIST} | tr -cd '[:alpha:]'`
LOGNAME=${PROJNAME}.${SLURM_JOBID}.log #N.B. This doesn't work yet
#this var only set inside SRUN `echo ${SLURM_JOB_NAME}|cut -d '-'`

#The shell command that gets parallelized.  Note your SCIRun path may differ.
COMMAND="${HOME}/SCIRun/${MACHINE}/Packages/Uintah/StandAlone/sus -mpi -mpmarches "

#Copy our input files and tables to the parallel FS.
mkdir -p /p/${FILESYSTEM}/${USER}/${PROJNAME}/logs
cd /p/${FILESYSTEM}/${USER}/${PROJNAME}
cp ~/${PROJNAME}/${PROJNAME}.ups .
cp ~/${PROJNAME}/*.mxn* .
cp ~/input.dtd .
rm -f current.log
ln -s logs/${LOGNAME} current.log

#Find a workable restart directory (if any) and use it.
dir=none
for dir in `ls -rd $PROJNAME.uda.*`
do
echo Moab job script: searching $dir for checkpoints.
  if [ `ls -d $dir/checkpoints/t*|wc -l` -ge 1 ]
  then
        break
  else  
        dir=none
  fi
done
echo Moab job script: Restarting from directory $dir .


# XXX Uintah has a problem with nuking the original inputfile.  Restore it from a backup.
if [ -e ${dir}/input.xml.orig ]
then
   cp -f ${dir}/input.xml.orig ${dir}/input.xml
fi

# Are we restarting?  Execute the appropriate parallel command here.
if [ -e ${PROJNAME}.uda -a ${dir} != "none" ]
then
  echo srun -n${SLURM_NTASKS} -N${SLURM_NNODES} ${COMMAND} -restart ${dir} 
  srun -n${SLURM_NTASKS} -N${SLURM_NNODES} ${COMMAND} -restart ${dir} 
else
  echo srun -n${SLURM_NTASKS} -N${SLURM_NNODES} ${COMMAND} ${PROJNAME}.ups
  srun -n${SLURM_NTASKS} -N${SLURM_NNODES} ${COMMAND} ${PROJNAME}.ups
fi

# All done!
